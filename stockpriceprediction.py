# -*- coding: utf-8 -*-
"""StockPricePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hDTlBP5H_EpF5ojg1pRo-N64Lix4taKk
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the Libraries
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib. pyplot as plt
import matplotlib
from sklearn. preprocessing import MinMaxScaler
from keras. layers import LSTM, Dense, Dropout
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib. dates as mandates
from sklearn. preprocessing import MinMaxScaler
from sklearn import linear_model
from keras. models import Sequential
from keras. layers import Dense
import keras. backend as K
from keras. callbacks import EarlyStopping
from keras. optimizers import Adam
from keras. models import load_model
from keras. layers import LSTM
from keras. utils import plot_model

#Get the Dataset
data=pd.read_csv("/content/LT.NS (1).csv",na_values=['null'],index_col='Date',parse_dates=True,infer_datetime_format=True)
data.head()

#Print the shape of Dataframe  and Check for Null Values
print('Dataframe Shape: ', data. shape)
print('Null Value Present: ', data.isnull().values.any())

data.info()

data.describe()

data.isnull().sum()

data = data.dropna()

data.shape

data=data.drop_duplicates()

data.shape

#Plot the True Adj Close Value
data['Adj Close'].plot()



#Set Target Variable
output_var = pd.DataFrame(data['Adj Close'])
#Selecting the Features
features = ['Open', 'High', 'Low', 'Volume']

#Scaling
scaler = MinMaxScaler()
feature_transform = scaler.fit_transform(data[features])
feature_transform= pd.DataFrame(columns=features, data=feature_transform, index=data.index)
feature_transform.head()

#Splitting to Training set and Test set
timesplit= TimeSeriesSplit(n_splits=10)
for train_index, test_index in timesplit.split(feature_transform):
        X_train, X_test = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]
        y_train, y_test = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()

#Process the data for LSTM
trainX =np.array(X_train)
testX =np.array(X_test)
X_train = trainX.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test = testX.reshape(X_test.shape[0], 1, X_test.shape[1])

model = Sequential()
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

from keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Implement learning rate scheduling
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

#Model Training
history=model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, shuffle=False)

#LSTM Prediction
y_pred= model.predict(X_test)

# Assuming you have already defined X_test and y_test

# Evaluate the model on the test data
test_loss = model.evaluate(X_test, y_test, verbose=0)

print(f'Test Loss (MSE): {test_loss:.4f}')

#Predicted vs True Adj Close Value â€“ LSTM
plt.plot(y_test, label='True Value')
plt.plot(y_pred, label='LSTM Value')
plt.title("Prediction by LSTM")
plt.xlabel('Time Scale')
plt.ylabel('Scaled USD')
plt.legend()
plt.show()